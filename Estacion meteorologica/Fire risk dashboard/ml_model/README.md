# ML Model Directory

This directory contains all machine learning components for the fire risk dashboard.

## Directory Structure

```
ml_model/
├── README.md                           # This file
├── QUICK_START_ML.md                   # Quick start guide
├── ML_IMPLEMENTATION_SUMMARY.md        # Complete documentation
│
├── prepare_training_data.py            # Step 1: Build training dataset
├── train_fire_model.py                 # Step 2: Train model
│
├── fire_model.pkl                      # Trained Random Forest model (included in repo)
├── training_data.csv                   # Training dataset (generated, gitignored - run prepare_training_data.py)
│
├── data/                               # Raw data (generated, gitignored)
│   └── cicatrices_incendios_resumen.geojson  # Historical fires from Dataverse (downloaded by prepare_training_data.py)
│
└── plots/                              # Evaluation plots
    ├── feature_importance.png          # Shows what ML learned
    ├── confusion_matrix.png            # Prediction accuracy breakdown
    └── roc_curve.png                   # Discrimination ability
```

## Quick Start

### 1. Run Dashboard (uses existing model)

From dashboard root:
```bash
conda activate fire_risk_dashboard
streamlit run app.py
```

The dashboard automatically loads `ml_model/fire_model.pkl` and displays ML predictions.

### 2. Retrain Model (from this directory)

```bash
cd ml_model

# Step 1: Prepare training data (~1-2 hours for full dataset)
python prepare_training_data.py

# Step 2: Train model (~5 minutes)
python train_fire_model.py

# Step 3: Return to root and restart dashboard
cd ..
streamlit run app.py
```

## Model Details

- **Algorithm**: Random Forest (100 trees)
- **Features**: temp_c, rh_pct, wind_kmh, days_no_rain (weather-only model)
- **Training data**: 616 fires + 616 non-fires from Araucania (2015-2024)
- **Current performance**: 80% accuracy, 0.85 ROC AUC

### Spatial Capabilities (Future Enhancement)

The current model uses **weather variables only** and predicts the same fire probability for any location with identical weather conditions. However, the training dataset (`training_data.csv`) **includes latitude and longitude** for each sample, enabling location-specific predictions.

**To add spatial awareness:**

1. The training data already contains `lat` and `lon` columns (lines 250-251 in `prepare_training_data.py`)
2. These spatial features are currently excluded during training (line 58 in `train_fire_model.py`)
3. To enable location-specific predictions, modify `train_fire_model.py`:

```python
# Change from:
feature_cols = ['temp_c', 'rh_pct', 'wind_kmh', 'days_no_rain']

# To:
feature_cols = ['temp_c', 'rh_pct', 'wind_kmh', 'days_no_rain', 'lat', 'lon']
```

4. Retrain the model with the new feature set
5. Update `app.py` to pass lat/lon when predicting

**Benefits of spatial model:**
- Learn geographic patterns (proximity to forests, topography via lat/lon, land use)
- Generate regional risk heat maps showing high-risk areas across Araucania
- Identify spatial risk factors beyond weather alone

**Visualization opportunities:**
- Heat map overlay on regional map showing ML-predicted risk across grid points
- Compare risk between different locations with same weather forecast
- Identify geographic hot spots for fire occurrence

## Configuration

Edit `prepare_training_data.py` to adjust:
- `MAX_SAMPLES`: Set to `None` for full dataset, or a number (e.g., 50) for quick testing
- `START_YEAR`, `END_YEAR`: Adjust time range
- `MIN_FIRE_SIZE_HA`: Filter by fire size

## Files Explanation

- **prepare_training_data.py**: Downloads historical fires, fetches weather data, creates balanced training set
- **train_fire_model.py**: Trains Random Forest, evaluates performance, saves model and plots
- **fire_model.pkl**: Trained model file loaded by main dashboard (`app.py`) - included in repo
- **training_data.csv**: Processed training dataset (temp, humidity, wind, dry days + fire/no-fire label) - **generated artifact, gitignored**
- **data/**: Raw fire dataset from Datos para Resiliencia - **generated artifact, gitignored** (downloads via `prepare_training_data.py`)
- **plots/**: Evaluation visualizations for presentations (generated by `train_fire_model.py`)

## Integration with Dashboard

The main dashboard (`../app.py`) loads the model:

```python
model = joblib.load("ml_model/fire_model.pkl")
```

For each day, it predicts fire probability:

```python
probability = model.predict_proba([[temp, humidity, wind, dry_days]])[0][1]
```

**Visualization:** The dashboard displays both risk metrics using dual semi-circular gauges that allow intuitive visual comparison between the rule-based risk index and ML fire probability. Color zones indicate risk levels (green/yellow/orange/red), and an agreement indicator shows whether the two methods are aligned.

## Statistical Validation

The dashboard includes statistical validation comparing the rule-based risk method and ML predictions. This validation uses actual historical fire data to determine if the two methods agree significantly.

### Running Validation

```bash
cd ml_model
python validate_model_agreement.py
```

This generates:
- `validation_results.json` - Statistical test results
- `plots/bland_altman.png` - Bland-Altman agreement plot

### Statistical Tests Performed

1. **McNemar's Test**: Tests for systematic disagreement between methods
   - p > 0.05 = no significant difference
   
2. **Concordance Correlation Coefficient (CCC)**: Measures agreement strength
   - ρc > 0.90 = strong agreement
   - ρc 0.75-0.90 = moderate agreement
   - ρc < 0.75 = poor agreement

3. **Bland-Altman Analysis**: Defines limits of agreement
   - Calculates mean difference and 95% confidence limits
   - Shows expected range of differences between methods

4. **Correlation Analysis**: Pearson and Spearman correlations

### Viewing Results in Dashboard

Click the "ℹ️" button next to the agreement indicator to view:
- Statistical test results
- Bland-Altman plot
- Interpretation of agreement status

The agreement indicator uses Bland-Altman limits (data-driven) rather than arbitrary thresholds.

### Re-running Validation

After retraining the model or updating training data:

```bash
python prepare_training_data.py  # Update training data
python train_fire_model.py        # Retrain model
python validate_model_agreement.py  # Re-run validation
```

### Mock vs Real Data

If `training_data.csv` doesn't exist, the validation script generates mock results for demonstration. These allow you to test the feature before running the time-intensive data preparation step.

## Creating a Spatial Model Variant

To experiment with spatial features while keeping the current weather-only model:

1. **Create a copy of the training script:**
   ```bash
   cp train_fire_model.py train_spatial_model.py
   ```

2. **Modify the feature list** in `train_spatial_model.py`:
   ```python
   feature_cols = ['temp_c', 'rh_pct', 'wind_kmh', 'days_no_rain', 'lat', 'lon']
   ```

3. **Save to a different filename:**
   ```python
   MODEL_OUTPUT_PATH = Path(__file__).parent / "fire_model_spatial.pkl"
   ```

4. **Train both models:**
   ```bash
   python train_fire_model.py          # Weather-only model
   python train_spatial_model.py       # Spatial model
   ```

5. **Compare performance** by reviewing the evaluation plots for each model

This approach lets you test spatial features without overwriting your current model. You can then choose which model performs better for your use case.

## See Also

- `QUICK_START_ML.md` — 5-minute test guide
- `ML_IMPLEMENTATION_SUMMARY.md` — Complete documentation with presentation tips
- `../README.md` — Main dashboard documentation

